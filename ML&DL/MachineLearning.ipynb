{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "$$y_{i} = \\mathbf{x}_{i}^{\\rm T} \\boldsymbol\\beta  + \\epsilon_{i}$$,\n",
    "$$\\epsilon_{i} \\sim N(0, \\sigma^2).$$ <br>\n",
    "$$\\rho(\\mathbf{y}|\\mathbf{X},\\boldsymbol\\beta,\\sigma^{2}) \\propto (\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{1}{2{\\sigma}^{2}}(\\mathbf{y}- \\mathbf{X} \\boldsymbol\\beta)^{\\rm T}(\\mathbf{y}- \\mathbf{X} \\boldsymbol\\beta)\\right)$$ <br>\n",
    "\n",
    "The ordinary least squares solution is to estimate the coefficient vector using the Moore-Penrose pseudoinverse:\n",
    "\n",
    "$${\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} )^{-1}\\mathbf {X} ^{\\rm {T}}\\mathbf {y} }  \\hat{\\boldsymbol\\beta} = (\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1}\\mathbf{X}^{\\rm T}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression (discrimitive)\n",
    "one vs one, y = 1, 2, 3, 4...     one vs all, argmax()   <br>\n",
    "[A good example](https://beckernick.github.io/logistic-regression-from-scratch/)\n",
    "$$z^{(i)} = w^T x^{(i)} + b $$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "- log- > converting multiplication into sum\n",
    "\n",
    "Backward Propagation: \n",
    "\n",
    "$$ dw = \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "## 2.1 Naive Bayes (generative, independent) (cannot handle missed category)(MLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Regulation\n",
    "In the ridge, the coefficients of the linear transformation are normal distributed and in the lasso they are Laplace distributed. In the lasso, this makes it easier for the coefficients to be zero and therefore easier to eliminate some of your input variable as not contributing to the output.\n",
    "\n",
    "<img src=\"images/lasso.png\"><img src=\"images/ridge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unbalanced Data set\n",
    "\n",
    "## 4.1 Data level\n",
    "- Random Undersampling aims to balance class distribution by randomly eliminating majority class examples. \n",
    "- Random Oversamping minority class  (overfitting)\n",
    "- Cluster based sampling, seperate them as majority class clusters and minority class clusters (overfitting)\n",
    "- Synthetic Minority Oversampling Algorithm (not effective for high dimensional data)\n",
    "- Modified synthetic minority oversampling technique (MSMOTE)\n",
    "\n",
    "## 4.2 Algorithmic ensembling \n",
    "\n",
    "Boosting (bias reduction for a simple model)\n",
    "```\n",
    "Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing).\n",
    "```\n",
    "Random Forest (bagging)(variance reduction for a complex model)(independent) (ID3, CART, C4.5)\n",
    "```\n",
    "Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification, and we say the tree \"votes\" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "```\n",
    "\n",
    "\n",
    "- Bagging Methodology (improved stability and accuracy, rduce variance, no overfitting, good for noisy) (can degrade)\n",
    "- <img src=\"images/bagging.png\" width=\"50%\" style=\"float:left;\">\n",
    "- Boosting\n",
    "- <img src=\"images/boosting.png\" width=\"50%\" style=\"float:left;\">\n",
    "    - Adaptive Boosting  (simple to implement) (sensitive to noisy data and outliers)\n",
    "    - Gradient Tree Boosting (harder to fit than random forests, 3 parameters: shrinkage, depth of tree and number of trees)\n",
    "    - XG Boost (XG Boost splits up to the maximum depth specified and prunes the tree backward and removes splits beyond which there is an only negative loss.)\n",
    "\n",
    "## 4.3 XGBoost\n",
    "(XGBoost can split approximately, instead of sorting all the probabilities)\n",
    "### 4.3.1 Features\n",
    "- Regularization\n",
    "- Parallel Processing (Rather it does the parallelization WITHIN a single tree my using openMP to create branches independently.)\n",
    "- Custimized objectives and evaluation criteria\n",
    "- Handling missing values\n",
    "- When reaching max_depth, start pruning the tree with negative gain\n",
    "- Build-in Cross-validation\n",
    "- Continue on existing model\n",
    "\n",
    "### [4.3.2 Parameters](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "1. Booster (tree based or linear model)   silent        nthread\n",
    "2. learning rate(eta), max_depth(default 6), max_lef_nodes(default 2^n), min_child_weight(control over fitting high-underfitting), gamma(loss reduction threshold)\n",
    "3. Objective(linear, logistic, softmax, softprob), eval_metrix(rmse, mae, logloss)\n",
    "\n",
    "\n",
    "1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample(% of data into subtree), colsample_bytree(% of data in level) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "3. Tune regularization parameters (lambda (L2regularization), alpha(L1regularization)) for xgboost which can help reduce model complexity and enhance performance.\n",
    " Lower the learning rate and decide the optimal parameters .\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SVM\n",
    "\n",
    "If the training data is **linearly separable**, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. <br>\n",
    "$y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\geq 1,\\quad {\\text{ for all }}1\\leq i\\leq n.\\qquad \\qquad $ <br>\n",
    "$\"Minimize {\\displaystyle \\|{\\vec {w}}\\|} \\|{\\vec {w}}\\| subject to {\\displaystyle y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\geq 1,} {\\displaystyle y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\geq 1,}   for {\\displaystyle i=1,\\,\\ldots ,\\,n} {\\displaystyle i=1,\\,\\ldots ,\\,n}\"$\n",
    "\n",
    "To extend SVM to cases in which the data are **not linearly separable**(but closed), we introduce the hinge loss function, <br>\n",
    "${\\displaystyle \\max \\left(0,1-y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\right)}$ <br> $minimize{\\displaystyle \\left[{\\frac {1}{n}}\\sum _{i=1}^{n}\\max \\left(0,1-y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\right)\\right]+\\lambda \\lVert {\\vec {w}}\\rVert ^{2},}$\n",
    "\n",
    "K(a,b) is the similarity between a and b <br>\n",
    "Else, introduce kernal function (map to higher dimensional features)\n",
    "## 5.1 Gaussian Kernal\n",
    "$(RBF Kernal){\\displaystyle k({\\vec {x_{i}}},{\\vec {x_{j}}})=\\exp(-\\gamma \\|{\\vec {x_{i}}}-{\\vec {x_{j}}}\\|^{2})}, for  {\\displaystyle \\gamma >0} \\gamma >0.$<br>$ Sometimes parametrized using {\\displaystyle \\gamma =1/{2\\sigma ^{2}}} \\gamma =1/{2\\sigma ^{2}}$\n",
    "\n",
    "$K({x_1},{x_2}) = \\exp ( - \\frac{{{{\\left\\| {{x_1} - {x_2}} \\right\\|}^2}}}{{2{\\sigma ^2}}}) = 1 + ( - \\frac{{{{\\left\\| {{x_1} - {x_2}} \\right\\|}^2}}}{{2{\\sigma ^2}}}) + \\frac{{{{( - \\frac{{{{\\left\\| {{x_1} - {x_2}} \\right\\|}^2}}}{{2{\\sigma ^2}}})}^2}}}{{2!}} + \\frac{{{{( - \\frac{{{{\\left\\| {{x_1} - {x_2}} \\right\\|}^2}}}{{2{\\sigma ^2}}})}^3}}}{{3!}} + \\cdots + \\frac{{{{( - \\frac{{{{\\left\\| {{x_1} - {x_2}} \\right\\|}^2}}}{{2{\\sigma ^2}}})}^n}}}{{n!}}$\n",
    "- gamma parameter defines how far the influence of a single training example reaches, ~ 1/radius (0.1, 1...)\n",
    "- C, missclassification, low C smooth, High C disjoint (1-10000)\n",
    "\n",
    "## 5.2 Factorization Machine (2010)\n",
    "1. Under sparse data (AD classification)\n",
    "2. linear complexity\n",
    "3. any real valued feature vector\n",
    "\n",
    "w(i,j) be the weight assigned to feature pair (i,j)\n",
    "<img src=\"images/factorial.png\", width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PCA(SVD), LDA\n",
    "**PCA** <br>\n",
    "U,S,V = svd(data) <br>\n",
    "data = data * u[:, 1:m]  <br>\n",
    "check S matrix, if selected samples has larger weight (>=0.99, 99% of variance retained) <br>\n",
    "\n",
    "**LDA** (Fisher's linear discriminant) <br>\n",
    "LDA approaches the problem by assuming that the conditional probability density functions ${\\displaystyle p({\\vec {x}}|y=0)} p({\\vec {x}}|y=0)$ and ${\\displaystyle p({\\vec {x}}|y=1)} p({\\vec {x}}|y=1)$ are both normally distributed with mean and covariance parameters ${\\displaystyle \\left({\\vec {\\mu }}_{0},\\Sigma _{0}\\right)} \\left({\\vec {\\mu }}_{0},\\Sigma _{0}\\right) and {\\displaystyle \\left({\\vec {\\mu }}_{1},\\Sigma _{1}\\right)} \\left({\\vec {\\mu }}_{1},\\Sigma _{1}\\right)$\n",
    "<br>\n",
    "$({\\vec {x}}-{\\vec {\\mu }}_{0})^{T}\\Sigma _{0}^{-1}({\\vec {x}}-{\\vec {\\mu }}_{0})+\\ln |\\Sigma _{0}|-({\\vec {x}}-{\\vec {\\mu }}_{1})^{T}\\Sigma _{1}^{-1}({\\vec {x}}-{\\vec {\\mu }}_{1})-\\ln |\\Sigma _{1}|\\ >\\ T$ <br>\n",
    "\n",
    "**Fisher Information** <br>\n",
    "The Fisher information is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ upon which the probability of X depends. <br>\n",
    "${\\displaystyle {\\mathcal {I}}(\\theta )=\\operatorname {E} \\left[\\left.\\left({\\frac {\\partial }{\\partial \\theta }}\\log f(X;\\theta )\\right)^{2}\\right|\\theta \\right]=\\int \\left({\\frac {\\partial }{\\partial \\theta }}\\log f(x;\\theta )\\right)^{2}f(x;\\theta )\\,dx}$ <br>\n",
    "**CRLB** = the inverse of fisher information matrix <br>\n",
    "$ \n",
    "\\mathcal{I}_{m,n}\n",
    "=\n",
    "\\frac{\\partial \\mu^\\mathrm{T}}{\\partial \\theta_m}\n",
    "\\Sigma^{-1}\n",
    "\\frac{\\partial \\mu}{\\partial \\theta_n}\n",
    "+\n",
    "\\frac{1}{2}\n",
    "\\operatorname{tr}\n",
    "\\left(\n",
    " \\Sigma^{-1}\n",
    " \\frac{\\partial \\Sigma}{\\partial \\theta_m}\n",
    " \\Sigma^{-1}\n",
    " \\frac{\\partial \\Sigma}{\\partial \\theta_n}\n",
    "\\right),\n",
    "$ <br>\n",
    "$\n",
    "\\frac{\\partial \\mu}{\\partial \\theta_m}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial \\mu_1}{\\partial \\theta_m} &\n",
    " \\frac{\\partial \\mu_2}{\\partial \\theta_m} &\n",
    " \\cdots &\n",
    " \\frac{\\partial \\mu_N}{\\partial \\theta_m}\n",
    "\\end{bmatrix}^\\mathrm{T};\n",
    "{\\displaystyle {\\frac {\\partial \\Sigma }{\\partial \\theta _{m}}}={\\begin{bmatrix}{\\frac {\\partial \\Sigma _{1,1}}{\\partial \\theta _{m}}}&{\\frac {\\partial \\Sigma _{1,2}}{\\partial \\theta _{m}}}&\\cdots &{\\frac {\\partial \\Sigma _{1,N}}{\\partial \\theta _{m}}}\\\\[5pt]{\\frac {\\partial \\Sigma _{2,1}}{\\partial \\theta _{m}}}&{\\frac {\\partial \\Sigma _{2,2}}{\\partial \\theta _{m}}}&\\cdots &{\\frac {\\partial \\Sigma _{2,N}}{\\partial \\theta _{m}}}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\{\\frac {\\partial \\Sigma _{N,1}}{\\partial \\theta _{m}}}&{\\frac {\\partial \\Sigma _{N,2}}{\\partial \\theta _{m}}}&\\cdots &{\\frac {\\partial \\Sigma _{N,N}}{\\partial \\theta _{m}}}\\end{bmatrix}}.} {\\displaystyle {\\frac {\\partial \\Sigma }{\\partial \\theta _{m}}}={\\begin{bmatrix}{\\frac {\\partial \\Sigma _{1,1}}{\\partial \\theta _{m}}}&{\\frac {\\partial \\Sigma _{1,2}}{\\partial \\theta _{m}}}&\\cdots &{\\frac {\\partial \\Sigma _{1,N}}{\\partial \\theta _{m}}}\\\\[5pt]{\\frac {\\partial \\Sigma _{2,1}}{\\partial \\theta _{m}}}&{\\frac {\\partial \\Sigma _{2,2}}{\\partial \\theta _{m}}}&\\cdots &{\\frac {\\partial \\Sigma _{2,N}}{\\partial \\theta _{m}}}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\{\\frac {\\partial \\Sigma _{N,1}}{\\partial \\theta _{m}}}&{\\frac {\\partial \\Sigma _{N,2}}{\\partial \\theta _{m}}}&\\cdots &{\\frac {\\partial \\Sigma _{N,N}}{\\partial \\theta _{m}}}\\end{bmatrix}}.}$\n",
    "\n",
    "\n",
    "**KL Divergence** <br>\n",
    "In mathematical statistics, the Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution diverges from a second, expected probability distribution.\n",
    "\n",
    "For discrete probability distributions P and Q, the Kullback–Leibler divergence from Q to P is defined[5] to be\n",
    "\n",
    "${\\displaystyle D_{\\mathrm {KL} }(P\\|Q)=-\\sum _{i}P(i)\\,\\log {\\frac {Q(i)}{P(i)}},} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7. ID3 Algorithm (entropy based on different input features vs output)\n",
    "- try to decrease the entropy\n",
    "[Pruning Algorithms Introduction](http://www.cnblogs.com/starfire86/p/5749334.html)\n",
    "\n",
    "decision tree from a dataset.\n",
    "1. Calculate the entropy of every attribute using the data set {\\displaystyle S} \n",
    "2. Split the set ${\\displaystyle S}$  into subsets using the attribute for which the resulting entropy (after splitting) is minimum (or, equivalently, information gain is maximum)\n",
    "3. Make a decision tree node containing that attribute\n",
    "4. Recurse on subsets using remaining attributes.\n",
    "<img src=\"images/decisiontree.png\", width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 K-Means \n",
    "- Unsupervised, self define K, for clustering. Clustering based on nearest centroid. Gradient descent.\n",
    "\n",
    "# 8.2 KNN\n",
    "- Supervised, used mostly for classification (some regression)\n",
    "- Count k nearest samples. Belong to the majority group.  (Can assign different weight for the sorted order)\n",
    "- k=5, weight = uniform/distance, algorithm = auto/ball_tree/kd_tree/brute_force\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
